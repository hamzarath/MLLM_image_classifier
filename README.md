# Gemma 3 is a neat freak! Exploring Image Classification with Gemma 3 12B Instruct
The power of modern language models isnâ€™t just in what they say â€” itâ€™s in how they understand. With the release of Gemma 3 12B Instruct, Google has made high-performance, open-weight AI accessible to anyone with a decent GPU. In this article, Iâ€™ll walk you through an experiment where I used Gemma to classify images of messy vs. clean rooms â€” with zero fine-tuning, running completely offline, and improved performance just by changing a few words in the prompt.

Letâ€™s start with what Gemma actually is â€” and why it matters.


ğŸ§  What Is Gemma 3 12B Instruct?
Gemma 3 12B Instruct is part of Googleâ€™s newly released Gemma family â€” a series of open-weight, instruction-tuned language models designed to be efficient, accessible, and safe.

The â€œ12Bâ€ stands for 12 billion parameters â€” the internal values that the model learns during training. These parameters allow the model to generate human-like responses, follow instructions, perform reasoning tasks, and more.

The Instruct variant of Gemma is fine-tuned specifically to follow natural language prompts, making it ideal for tasks like Q&A, summarization, and task execution â€” without needing any additional training. Itâ€™s built to understand what you're asking and respond accordingly.

One of Gemmaâ€™s biggest strengths? Itâ€™s lightweight and designed to run on consumer-grade GPUs. You donâ€™t need a data center to use it â€” models like Gemma 3 12B Instruct can run locally and offline, which is a game-changer for privacy-conscious developers and rapid experimentation.

Link to the original announcement for more details: Introducing Gemma 3


ğŸ§ª The Use Case: Classifying Room Images with Just a Prompt
The goal of this experiment was simple: Can a language model â€” not specifically trained for image classification â€” determine whether a room is messy or clean just by looking at a picture?

To test this, I used the Messy vs. Clean Room dataset on Kaggle. This dataset contains labeled images of bedrooms and other spaces, each tagged as either â€œcleanâ€ or â€œmessy.â€ Itâ€™s typically used to train computer vision models, but I wanted to see how far we could go using only prompting and a general-purpose LLM.

Instead of building or training a specialized vision model, I simply passed each image to Gemma 3 12B Instruct, and gave it a basic task:

Classify the image as 'clean' or 'messy'. Reply only with 'clean' or 'messy'.

No fine-tuning. No additional data. Just a prompt and a powerful model running locally.


ğŸ’» Experimenting with Gemma Using LM Studio
To run Gemma 3 12B Instruct locally, I used LM Studio â€” a desktop application that makes it easy to download, load, and interact with large language models on your own machine.

ğŸ§° What Is LM Studio?
A free, cross-platform app for running open-source LLMs locally

Supports GGUF models via the llama.cpp backend (efficient CPU/GPU inference)

Offers both a chat interface and a local API server for programmatic interaction

I initially used the built-in chat interface (screenshot below) to prompt Gemma with images and collect responses. This made it easy to test variations of the task manually before moving to automated experiments via Python.

Minimize image
Edit image
Delete image

Testing Gemma 3 using the Chat interface of LM Sudio


LM Studio also allows you to run a local HTTP server, enabling integration with custom scripts. This is how I connected Python to automate the image classification process and evaluate performance at scale.

ğŸ‘‰ You can download LM Studio here

ğŸ“˜ Tutorial: Mastering LM-Studio: Unleashing LLMs locally


ğŸ› ï¸ My Setup: Running Gemma Locally
One of the most exciting parts of this project is that everything ran offline, on my own machine â€” no cloud GPUs, no API keys, and no internet required.

My GPU is an NVIDIA RTX 4070 Ti Super with 16 GB of VRAM â€” enough to comfortably run Gemma 3 12B Instruct in GGUF format (Q4_K_M quantized).

Quantization is a technique that reduces the size of a model by lowering the precision of its internal weights (e.g., from 16-bit floats to 4-bit integers). This drastically cuts down memory requirements while keeping performance nearly intact. Thanks to quantization, we are able to run parameter-heavy models locally without specialized hardware.

Why GPU & VRAM Matter:
GPU Acceleration drastically speeds up inference compared to CPU-only setups, making interactions with the model feel real-time.

VRAM: While your systemâ€™s RAM handles general-purpose tasks, VRAM (Video RAM) is dedicated memory on the GPU. Itâ€™s where the model is actually loaded during inference. If the model doesn't fit in VRAM, it has to spill over to system RAM or disk â€” which slows things down dramatically, or makes it impossible to run altogether.


ğŸ“ˆ Initial Results: What Happened Out of the Box?
With the setup ready and the dataset in place, I ran the first round of image classification using Gemma 3 12B Instruct. I passed one image at a time through LM Studio, each paired with the following prompt:

Classify the image as 'clean' or 'messy'. Reply only with 'clean' or 'messy'.

No context. No examples. Just the image and the instruction.

The Results:

Precision (clean): 1.00 â†’ Every time it said â€œclean,â€ it was correct

Recall (clean): 0.79 â†’ It missed some clean rooms

Precision (messy): 0.83 â†’ Some clean rooms were wrongly called messy

Recall (messy): 1.00 â†’ It caught all the messy rooms

My Intuition:

Apparently, Gemma is a bit of a neat freak â€” it was quick to call out mess, even if the room just looked slightly untidy. That behavior makes sense though: the model hasn't been trained for this specific visual judgment. The fact that it performed this well, with no training and just a prompt, was already impressive.


âš ï¸ The Results Were Good â€” But Not Good Enough
The initial performance was solid: perfect precision for clean rooms, and perfect recall for messy ones. But still, something felt off.

Gemma was missing some clean rooms and occasionally flagging them as messy. For a human, these rooms clearly werenâ€™t disasters â€” but to Gemma, a slightly rumpled bed mightâ€™ve been enough to trigger the "messy" label.

I wanted to improve the results. A few years ago, this is what that wouldâ€™ve involved:

Improving Model Performance Used to Mean:

Collecting more labeled training data

Writing a custom training pipeline (usually in PyTorch or TensorFlow)

Fine-tuning the model with careful hyperparameter tuning

Monitoring loss curves and overfitting risks

Running experiments for hours or days on a GPU cluster

Deploying and re-evaluating over and over again

Even small improvements required serious time, compute, and expertise.

But in this case, I didnâ€™t touch the model at all. I just rewrote the prompt.


âœï¸ The Fix? Just a Better Prompt
Instead of retraining or changing anything under the hood, I decided to change the instruction I gave the model. Here's what I started with:

Classify the image as 'clean' or 'messy'. Reply only with 'clean' or 'messy'.

It was short, clear, and objective â€” but maybe too strict. So I added just a bit of guidance:

Classify the image as 'clean' or 'messy'. Be less neat and more tolerant. Do not classify the room as messy unless it is truly messy. Reply only with 'clean' or 'messy'.

That small change made a big difference.

New Results â€” Perfect Accuracy:

Precision (clean): 1.00

Recall (clean): 1.00

Precision (messy): 1.00

Recall (messy): 1.00

The model no longer misjudged clean rooms, and it still confidently identified messy ones. All it needed was a clearer set of expectations â€” no code, no data, no retraining.

Disclaimer: Keep in mind that this is a curated Kaggle dataset, where the images can clearly be classified into the 2 chosen classes. One should not expect this high of a performance on other datasets.


âœ¨ Why This Is Incredible
This wasnâ€™t just a fun test â€” itâ€™s a glimpse into the future of how we build with AI.

Hereâ€™s why this matters:
No training required: Gemma wasnâ€™t fine-tuned or retrained. It understood the task through instructions alone.

Performance improved with a few words: A simple prompt tweak led to perfect accuracy â€” no models were changed.

30 minutes, start to finish: From setup to final result, the entire process took less than half an hour.

General-purpose model: Gemma isnâ€™t built just for image tasks â€” it can answer questions, write code, and more.

Runs offline on mainstream hardware: All of this happened locally, on a consumer GPU, with no cloud or internet dependency.

The Big Shift:

What used to take days of engineering and training can now be achieved by writing a better sentence. This is more than just a productivity gain â€” itâ€™s a shift in how we solve problems with AI.


ğŸ§© Other Use Cases: One Model, Many Applications
While this experiment focused on classifying messy vs. clean rooms, the real power of Gemma 3 12B Instruct lies in its flexibility. With the ability to understand both language and images, it can be applied across a variety of practical scenarios â€” no retraining required.

Real-World Use Cases:
ğŸ¬ Warehouse Monitoring Identify stock levels, misplaced items, or empty shelves using visual input from camera feeds.

ğŸ§° Telco Field Interventions Perform post-task verification by asking the model: â€œIs this site cleaned and closed properly?â€ â€” based on a single image.

ğŸ¥ Healthcare & Workplace Safety Assist in monitoring hospital rooms or workspaces to ensure they meet cleanliness and safety standards.

All of these tasks can be handled using the same model, the same hardware, and just the right prompt.
